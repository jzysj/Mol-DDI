
import yaml
import numpy as np
import os
import shutil
import sys
import torch
import yaml
from tes import NTXentLoss
import torch.nn.functional as F
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import CosineAnnealingLR
from datetime import datetime
# 导入dataset 类
# from dataset import MoleculeDatasetWrapper
from five_process_pretrain import MoleculeDatasetWrapper
from dataset_pretrain import myIterableDataset
# 导入模型GAT类
# from gat import GATNet
# from gcn_pretrain import GCNNet
from five_pretrian import GCNNet
# from only_one_encode_pretrain import GCNNet
# 是否使用半精度训练模型

apex_support = False
from torch_geometric.loader import DataLoader

def _save_config_file(model_checkpoints_folder):
    if not os.path.exists(model_checkpoints_folder):
        os.makedirs(model_checkpoints_folder)
        # 返回复制之后的文件路径
        shutil.copy('./config.yaml', os.path.join(model_checkpoints_folder, 'config.yaml'))


# 写一个类用于模型的训练
class MOSS(object):
    def __init__(self, train_loader, valid_loader, config):
        self.config = config
        self.device = self._get_device()
        self.train_loader = train_loader
        self.valid_loader = valid_loader
        # 当前时间
        dir_name = datetime.now().strftime('%b%d_%H-%M-%S')
        # 日志路径，用于tensorboard
        log_dir = os.path.join('ckpt', dir_name)

        self.writer = SummaryWriter(log_dir=log_dir)

        # 定义损失类
        self.nt_xent_criterion = NTXentLoss(self.device, config['batch_size'], **config['loss'])

    # 获得训练的GPU
    def _get_device(self):
        if torch.cuda.is_available() and self.config['gpu'] != 'cpu':
            device = self.config['gpu']
            torch.cuda.set_device(device)
        else:
            device = 'cpu'
        print("Running on:", device)
        return device

    # 模型迭代
    def _step(self, model, x1s1, x1s2, x2s1, x2s2, x3s1, x3s2, x4s1, x4s2, x5s1, x5s2, n_iter):
        # get the representations and the projections
        out1, out2, out3, out4, out5, out6, out7, out8, out9, out10 = model(x1s1, x1s2, x2s1, x2s2, x3s1, x3s2, x4s1, x4s2, x5s1, x5s2)  # [N,C]
        # get the representations and the projections
        # rjs, zjs = model(xjs)  # [N,C]

        # normalize projection feature vectors
        out1 = F.normalize(out1, dim=1)
        out2 = F.normalize(out2, dim=1)

        out3 = F.normalize(out3, dim=1)
        out4 = F.normalize(out4, dim=1)

        out5 = F.normalize(out5, dim=1)
        out6 = F.normalize(out6, dim=1)

        out7 = F.normalize(out7, dim=1)
        out8 = F.normalize(out8, dim=1)

        out9 = F.normalize(out9, dim=1)
        out10 = F.normalize(out10, dim=1)
        loss = self.nt_xent_criterion(out1, out2)+self.nt_xent_criterion(out3, out4)+self.nt_xent_criterion(out5, out6)+\
               self.nt_xent_criterion(out7, out8)+self.nt_xent_criterion(out9, out10)
        return loss

    def train(self):
        train_loader, valid_loader = self.train_loader, self.valid_loader
        model = GCNNet().to(self.device)  # 初始化模型
        # print(model)

        # 定义优化函数
        optimizer = torch.optim.Adam(
            model.parameters(), self.config['init_lr'],
            weight_decay=eval(self.config['weight_decay'])
        )

        # 余弦退火策略
        scheduler = CosineAnnealingLR(
            optimizer, T_max=self.config['epochs']-self.config['warm_up'],
            eta_min=0, last_epoch=-1
        )
        # 使用半精度训练模型
        # if apex_support and self.config['fp16_precision']:
        #     model, optimizer = amp.initialize(
        #         model, optimizer, opt_level='O2', keep_batchnorm_fp32=True
        #     )

        model_checkpoints_folder = os.path.join(self.writer.log_dir, 'checkpoints')
        # 保存配置文件
        _save_config_file(model_checkpoints_folder)

        n_iter = 0
        valid_n_iter = 0
        best_valid_loss = np.inf  # 正无穷，类型为浮点型
        for epoch_counter in range(self.config['epochs']):
            for bn, (x1s1, x1s2, x2s1, x2s2, x3s1, x3s2, x4s1, x4s2, x5s1, x5s2) in enumerate(train_loader):
                optimizer.zero_grad()  # 梯度归0
                x1s1 = x1s1.to(self.device)
                x1s2 = x1s2.to(self.device)

                x2s1 = x2s1.to(self.device)
                x2s2 = x2s2.to(self.device)

                x3s1 = x3s1.to(self.device)
                x3s2 = x3s2.to(self.device)

                x4s1 = x4s1.to(self.device)
                x4s2 = x4s2.to(self.device)

                x5s1 = x5s1.to(self.device)
                x5s2 = x5s2.to(self.device)
                loss = self._step(model, x1s1, x1s2, x2s1, x2s2, x3s1, x3s2, x4s1, x4s2, x5s1, x5s2, n_iter)
                # 每50次保存一下loss
                if n_iter % self.config['log_every_n_steps'] == 0:
                    self.writer.add_scalar('train_loss', loss, global_step=n_iter)
                    self.writer.add_scalar('cosine_lr_decay', scheduler.get_last_lr()[0], global_step=n_iter)
                    print(epoch_counter, bn, loss.item())

                # 是否使用半精度训练
                # if apex_support and self.config['fp16_precision']:
                #     # with amp.scale_loss(loss, optimizer) as scaled_loss:
                #     #     scaled_loss.backward()
                # else:

                loss.backward()

                optimizer.step()
                n_iter += 1

                # 验证模型,每个epoch 验证模型
            if epoch_counter % self.config['eval_every_n_epochs'] == 0:
                valid_loss = self._validate(model, valid_loader)
                print(epoch_counter, bn, valid_loss, '(validation)')
                if valid_loss < best_valid_loss:
                    # 保存模型权重
                    torch.save(model.state_dict(), os.path.join(model_checkpoints_folder, 'model.pth'))
                self.writer.add_scalar('validation_loss', valid_loss, global_step=valid_n_iter)
                valid_n_iter += 1
            # 每5次迭代保存模型
            if (epoch_counter + 1) % self.config['save_every_n_epochs'] == 0:
                torch.save(model.state_dict(),
                           os.path.join(model_checkpoints_folder, 'model_{}.pth'.format(str(epoch_counter))))

            if epoch_counter >= self.config['warm_up']:
                scheduler.step()

    def _load_pre_trained_weights(self, model):
        try:
            checkpoints_folder = os.path.join('./ckpt', self.config['load_model'], 'checkpoints')
            state_dict = torch.load(os.path.join(checkpoints_folder, 'model.pth'))
            model.load_state_dict(state_dict)
            print("Loaded pre-trained model with success.")
        except FileNotFoundError:
            print("Pre-trained weights not found. Training from scratch.")

    def _validate(self, model, valid_loader):
        # validation steps
        with torch.no_grad():
            model.eval()

            valid_loss = 0.0
            counter = 0
            for (x1s1, x1s2, x2s1, x2s2, x3s1, x3s2, x4s1, x4s2, x5s1, x5s2) in valid_loader:
                x1s1 = x1s1.to(self.device)
                x1s2 = x1s2.to(self.device)

                x2s1 = x2s1.to(self.device)
                x2s2 = x2s2.to(self.device)

                x3s1 = x3s1.to(self.device)
                x3s2 = x3s2.to(self.device)

                x4s1 = x4s1.to(self.device)
                x4s2 = x4s2.to(self.device)

                x5s1 = x5s1.to(self.device)
                x5s2 = x5s2.to(self.device)

                loss = self._step(model, x1s1, x1s2, x2s1, x2s2, x3s1, x3s2, x4s1, x4s2, x5s1, x5s2, counter)

                valid_loss += loss.item()
                counter += 1
            valid_loss /= counter

        model.train()
        return valid_loss


def main():
    config = yaml.load(open("config.yaml", "r"), Loader=yaml.FullLoader)
    # print(config)

    # dataset_train = myIterableDataset(file_path_train)
    # dataset_valid = myIterableDataset(file_path_valid)

    # train_loader = DataLoader(dataset_train, batch_size=config['batch_size'], shuffle=False, drop_last=True)
    # valid_loader = DataLoader(dataset_valid, batch_size=config['batch_size'], shuffle=False, drop_last=True)
    dataset = MoleculeDatasetWrapper(config['batch_size'], **config['dataset'])
    train_loader, valid_loader = dataset.get_data_loaders()
    moss = MOSS(train_loader, valid_loader, config)
    moss.train()


if __name__ == '__main__':

    main()
